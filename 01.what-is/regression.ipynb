{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64342b2-1033-4ec0-b67d-30c2ef9c52f3",
   "metadata": {},
   "source": [
    "# what is machine learning?\n",
    "\n",
    "In this exercise, we'll get a look into some of the basics of machine learning, including:\n",
    "\n",
    "- training a model\n",
    "- loss and loss functions\n",
    "- learning rates and epochs\n",
    "- scaling data\n",
    "\n",
    "To illustrate these, we'll use a simple model that we've seen plenty of times before: a linear model with a single variable (feature). Using machine learning for this sort of problem is largely overkill, but it is useful to help illustrate some of the points with a familiar example.\n",
    "\n",
    "## data\n",
    "\n",
    "The data used in this exercise are the historic meteorological observations from the [Armagh Observatory](https://www.metoffice.gov.uk/weather/learn-about/how-forecasts-are-made/observations/recording-observations-for-over-100-years), downloaded from the [UK Met Office](https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data).\n",
    "\n",
    "To make the data slightly easier to work with, I have done the following:\n",
    "- Removed the header on lines 1-5\n",
    "- Replaced multiple spaces with a single space, and replaced single spaces with a comma (`,`)\n",
    "- Removed `---` to indicate no data, leaving these fields blank\n",
    "- Removed `*` indicating provisional/estimated values\n",
    "- Removed the 2023 data\n",
    "- Renamed the file `armaghdata.csv`.\n",
    "\n",
    "If you wish to use your own data (and there are loads of stations available!), please feel free. \n",
    "## importing libraries\n",
    "\n",
    "Before getting started, we will import the libraries (packages) that we will use in the exercise:\n",
    "\n",
    "- [sklearn](https://scikit-learn.org/), for fitting a linear model to our data;\n",
    "- [pandas](https://pandas.pydata.org/), for reading the data from a file;\n",
    "- [numpy](https://numpy.org/), for working with arrays;\n",
    "- [matplotlib](https://matplotlib.org/), for making plots.\n",
    "\n",
    "Remember that to do this, we use the `import` statement, followed by the name of the package. We can also use `from` to import part of a package, and we can *alias* the package name using `as`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb92e3-ade1-4dca-9943-1385cdab061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9b2a6-6bbc-48d5-8535-ffa5a0d7ae90",
   "metadata": {},
   "source": [
    "Next, let's read in the first dataset that we will use for the exercise, using `pd.read_csv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327f922-ec3f-4a9e-b0fe-b163b1c4b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/armaghdata.csv') # read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bc58c-6f0d-432a-b124-77c709005ba0",
   "metadata": {},
   "source": [
    "For this exercise, we're going to look at the relationship between monthly hours of sun and the monthly maximum temperature. Before we begin with linear regression, we can use the`.plot.scatter()` method [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html) with our **DataFrame** to show a scatter plot of these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0008f-8844-4fa8-95ce-1ba156fdfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter('sun', 'tmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9289dd-7cd4-4cf7-9bba-13020e9aaa1c",
   "metadata": {},
   "source": [
    "From this plot, we can see that there is an *approximately* linear relationship between these two variables - at least, enough so that we can use it to help us ease into some of the core concepts of machine learning. \n",
    "\n",
    "## linear regression\n",
    "\n",
    "Remember that a linear model with a single variable has the form:\n",
    "\n",
    "$$ y = \\beta + \\alpha x $$\n",
    "\n",
    "where $\\beta$ is the *intercept* of the line and $\\alpha$ is the *slope* of the line. \n",
    "\n",
    "The terminology often used in machine learning is a little bit different. The equation for a linear model is often written as:\n",
    "\n",
    "$$ \\hat{y} = b + wx $$\n",
    "\n",
    "where $\\hat{y}$ is the **predicted** value/label, $b$ is the **bias**, and $w$ is the **weight** of the **feature** $x$. Extending this to multiple features (variables), the form looks like:\n",
    "\n",
    "$$ \\hat{y} = b + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n = b + \\sum_i w_i x_i $$\n",
    "\n",
    "Where each feature $x_i$ has a corresponding weight $w_i$.\n",
    "\n",
    "For this first example, we will look at a model with a single feature (variable): the relationship between `sun` and `tmax`. To use `scikit-learn` for linear regression, we first create a **LinearRegression** object ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd3498-6c57-4aac-87dc-4f9636d42412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a LinearRegression object\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821aa504-85e9-4179-8e7e-c4fe6e6a8316",
   "metadata": {},
   "source": [
    "Next, we can prepare our data. For fitting data with `scikit-learn`, we need to make sure that we have dropped all `NaN` values from the data we want to fit. We also need to reshape each array so that they have shape $N\\times 1$ (or, for multiple linear regression, $N\\times m$, where $m$ is the number of explanatory variables we are using for the fit).\n",
    "\n",
    "To do this, we first use `.dropna()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)) along with the `subset` argument to remove all `NaN` values from the `sun` and `tmax` columns. \n",
    "\n",
    "Then, we use the `.to_numpy()` method ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_numpy.html)) to get the values of each **Series** as an array, before using the `.reshape()` method ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html)) on the resulting `numpy` **ndarray** to reshape the array so that it is the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0c3e9-c5ea-46fb-ba78-f5c00301cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where either sun or tmax is NaN\n",
    "data = data.dropna(subset=['sun', 'tmax'], how='any')\n",
    "\n",
    "sun = data['sun'].to_numpy().reshape(-1, 1) # reshape so data are N x 1\n",
    "tmax = data['tmax'].to_numpy().reshape(-1, 1) # reshape so data are N x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf669be-f5c9-4c9a-ac2a-465332d8a4d4",
   "metadata": {},
   "source": [
    "Now that we have the data prepared, we can use the `.fit()` method ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit)) of the **LinearRegression** object to calculate the parameters of the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745736a-c7a4-49c2-b488-3755e14ec88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sun, tmax) # fit the linear model to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e86def-ef24-4d1f-8830-6b434e03d1da",
   "metadata": {},
   "source": [
    "The value of the weight(s) (slope) is stored in the `.coef_` attribute of the **LinearRegression** object, and the value of the bias (intercept) is stored in the `.intercept_` attribute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bac08d-6534-488f-9b90-e7393e30831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_ # show the coefficient(s) and intercept for the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101c5d1-e0e7-4b3f-aef5-99da0b22516c",
   "metadata": {},
   "source": [
    "From the above, we can see the weight for the `sun` feature is 0.06515, and the value of the bias is 6.34858. To calculate the predicted value of the model for new features, we can use the `.predict()` method ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict)). And, using `matplotlib`, we can show the fitted model alongside the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213bfb7-3a1a-41f9-b96d-64c4c16077f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0, 300, 5) # create an array of 5 values from 0 to 300, spaced evenly\n",
    "\n",
    "fig, ax = plt.subplots(1, 1) # create a figure with a single axis\n",
    "\n",
    "ax.plot(sun, tmax, 'k.', label='data') # plot tmax vs sun as black dots with a label 'data'\n",
    "ax.plot(xx, model.predict(xx.reshape(-1, 1)), 'r--', label='linear fit') # plot the regression line as a red dashed line with label 'linear fit'\n",
    "\n",
    "ax.legend() # show the legend\n",
    "\n",
    "ax.set_xlabel('hours of sun') # set the xlabel of the axis\n",
    "ax.set_ylabel('monthly maximum temperature (Â°C)') # set the ylabel of the axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65273fa6-a6ff-4b08-921d-fecd025de010",
   "metadata": {},
   "source": [
    "## training and loss\n",
    "\n",
    "In machine learning, the difference between the predicted value/label and the measured value/label is called **loss**. The goal with machine learning is to find values for the model **parameters** (e.g., weights and biases) that minimizes the total or average loss for all examples - that is, we want the vertical distance between each of our observations and the regression line to be as small as possible, on average.\n",
    "\n",
    "We can calculate the loss for our model by first using the `.predict()` method ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict)) to get the predicted value for each feature value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce82ec5-b39c-4746-98aa-1affac110880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the fitted parameters to get the predicted values at the input x data\n",
    "predicted = model.predict(sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3451e4-8ee8-44fd-82bd-a1343df87bec",
   "metadata": {},
   "source": [
    "Now, we can plot the value of loss for each input feature value, as a function of the predicted value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c67d7f-9679-481b-97c4-92c172ce7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss as difference between observed, predicted values\n",
    "loss = tmax - predicted\n",
    "\n",
    "fig, ax = plt.subplots(1, 1) # create a new figure and axis\n",
    "\n",
    "ax.axhline(y=0, xmin=loss.min(), xmax=loss.max(), color='k', linestyle='--') # plot a horizontal line at loss = 0\n",
    "ax.plot(predicted, loss, 'o') # plot the loss as a function of the predicted value\n",
    "\n",
    "ax.set_xlabel('predicted value')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e95c3-dec8-4cfb-8ecd-06ba39669cd7",
   "metadata": {},
   "source": [
    "From the figure above, we can see that that the loss for our linear model ranges between -8 and 8, with most of the values falling between about -4 and 4.\n",
    "\n",
    "In practice, finding the model parameters that minimize the total or average loss for all examples means finding the minimum values of a **cost** or **objective function** (or **loss function**) - that is, a function that allows us to calculate the total or average loss for all of our input data.\n",
    "\n",
    "One commonly used loss function is known as **squared error loss** (or **L$_2$ loss**), which calculates the squared difference between the label and the predicted value:\n",
    "\n",
    "$$(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "The average loss for all of the input data, or the **mean squared error** (**MSE**), is calculated as:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "We can calculate and print the value of this loss using the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37da57e-2466-43b8-9ab7-1e09a1f7e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss = (loss**2).mean() # calculate the mean of the squared loss\n",
    "print(f'MSE: {average_loss:.2f}') # print the value of the average loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605232a4-a3ad-416d-9fa0-fa1a78ad20f3",
   "metadata": {},
   "source": [
    "## gradient descent and learning\n",
    "\n",
    "The way that we go about finding the minimum value of a function is by taking the gradient of the function and setting it to zero. The reason that we use the squared loss, rather than something like the absolute value, is because the derivatives of the squared loss are easy to calculate and behave nicely (as opposed to absolute values or cubic or quartic functions). Depending on the function, we might even be able to find a *closed form* solution to the problem - that is, a way to calculate the optimum parameter values directly. In fact, this is what has been implemented in `LinearRegression.fit()`: the solution returned by `.fit()` is found using an approach called Ordinary Least Squares regression - it's not actually a machine learning approach at all.\n",
    "\n",
    "When we don't have a nice *closed form* solution to the minimization problem, which is very often the case, we use some kind of numerical optimization method in order to find a solution. One of the more common approaches that you might come across is something called **gradient descent**. \n",
    "\n",
    "Assuming that our cost function is differentiable, we can travel in the opposite direction of the gradient at a given point (i.e., the slope of a line or a surface) in order to find a minimum value of the function.\n",
    "\n",
    "To do this, we first need to take the partial derivatives of the cost function $l$ with respect to our model parameters:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial w} = \\frac{1}{N} \\sum_i -2x_i (y_i - (wx_i + b)) $$\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial b} = \\frac{1}{N} \\sum_i -2(y_i - (wx_i + b)) $$\n",
    "\n",
    "We then iterate over a number of **epochs** (steps) in order to find a solution, by calculating the value of the partial derivatives at each point and subtracting a multiple of this value from the current estimate of each parameter:\n",
    "\n",
    "$$ w_{i + 1} = w_i - \\alpha \\frac{\\partial l}{\\partial w}(w_i, b_i) $$\n",
    "\n",
    "$$ b_{i + 1} = b_i - \\alpha \\frac{\\partial l}{\\partial b}(w_i, b_i) $$\n",
    "\n",
    "where the **learning rate** $\\alpha$ controls the amount by which we update the parameter values at each step. By subtracting the value of the partial derivative, we ensure that we are always moving toward a 0 value of the partial derivative. \n",
    "\n",
    "Over the next several blocks of code, we'll see how we can implement this before investigating how changing **hyperparameters** like the learning rate or the number of epochs affects the solution we are able to find.\n",
    "\n",
    "First, we'll define a function, `update_parameters()`, that calculates the new value of $w$ and $b$ based on the learning rate and partial derivatives of the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281132f-700a-4196-8ef5-5566e86b67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(xdata, ydata, weight, bias, learning_rate):\n",
    "    dl_dw = (-2 * xdata * (ydata - (weight * xdata + bias))).mean() # calculate the partial derivative of l wrt w\n",
    "    dl_db = (-2 * (ydata - (weight * xdata + bias))).mean() # calculate the partial derivative of l wrt b\n",
    "\n",
    "    weight -= dl_dw * learning_rate # subtract dl/dw * learning_rate from w\n",
    "    bias -= dl_db * learning_rate # subtract dl/db * learning_rate from b\n",
    "\n",
    "    return weight, bias # return the updated values of w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd34872-6122-4c93-b94a-69b10ddf5394",
   "metadata": {},
   "source": [
    "We can also define a function, `predict()`, to help us calculate $\\hat{y}$ for a given value of $w$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6b022-832e-4f8d-b85f-4b1fd66e5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xdata, weight, bias):\n",
    "    return weight * xdata + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e08f4-bced-44fb-8aa9-58f97201cc84",
   "metadata": {},
   "source": [
    "along with the loss function, which calculates the mean squared error for the given values of $w$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0e8c6-fddb-471f-b371-4e59dbc2b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_loss(xdata, ydata, weight, bias):\n",
    "    loss = (ydata - predict(xdata, weight, bias))**2 \n",
    "    return loss.mean() # return the mean of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1369fb-3d97-4abf-9e8c-f46bc9421e26",
   "metadata": {},
   "source": [
    "And finally, we can write a function, `train()`, to get to the \"best\" values of $w$ and $b$. \n",
    "\n",
    "In this example, I am saving the values to a **DataFrame** every 10 epochs to help cut down on the number of values. At the end, I also create a plot showing the loss value as a function of the epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23854bdc-3aa4-4c76-a56a-24d35d419da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xdata, ydata, weight, bias, learning_rate, epochs, plot=True):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for ee in range(epochs):\n",
    "        weight, bias = update_parameters(xdata, ydata, weight, bias, learning_rate)\n",
    "\n",
    "        if ee % 10 == 0:\n",
    "            df.loc[ee, 'weight'] = weight\n",
    "            df.loc[ee, 'bias'] = bias\n",
    "            df.loc[ee, 'avg_loss'] = avg_loss(xdata, ydata, weight, bias)\n",
    " \n",
    "    df.loc[ee, 'weight'] = weight\n",
    "    df.loc[ee, 'bias'] = bias\n",
    "    df.loc[ee, 'avg_loss'] = avg_loss(xdata, ydata, weight, bias)\n",
    "\n",
    "    # plot the value of the average loss for each epoch\n",
    "    if plot:\n",
    "        ax = df.reset_index(names='epoch').plot('epoch', 'avg_loss', legend=False)\n",
    "        ax.set_ylabel('average loss')\n",
    "        \n",
    "    \n",
    "    return df.reset_index(names=['epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7cd28-af29-4ded-abe4-0d5302494729",
   "metadata": {},
   "source": [
    "Now that we have defined the functions that we need in order to train the model, let's try this out. Running the cell below will train the model for 10,000 epochs, with a very small learning rate ($10^{-8}$). At the end, we use `.tail()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html)) to view the parameter values and the average loss for the model at the final step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714e15a-da9b-4415-8b09-88cd9f247cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "output = train(sun, tmax, weight=0, bias=6.34, learning_rate=1e-7, epochs=10000)\n",
    "\n",
    "output.tail(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742214b4-f0bc-40aa-8889-657f79879105",
   "metadata": {},
   "source": [
    "We can see that after 10,000 epochs, the parameter values have gotten close to the optimal values found using `LinearRegression.fit()`, though we're not quite there. We can also see that the average loss is low, but not quite to the level we calculated for those optimum values. Try changing the learning rate to a larger value - say, $10^{-6}$ or $10^{-5}$ - how does this impact the shape of the loss curve? What about the value for the weight and bias?\n",
    "\n",
    "What about larger values of learning rate, like 0.01?\n",
    "\n",
    "Once you have tested difference learning rate values, re-run the cell above with the original value (`1e-7`), then run the cell below to plot the regression line using the values for each epoch. You should notice that there's a big jump from the initial value, then a much slower change as the model converges toward the optimal values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31031b37-f94f-472c-807e-3adf625cc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(sun, tmax, '.')\n",
    "\n",
    "xx = np.arange(0, 351, 50)\n",
    "\n",
    "for ind in [0, 10, 50, 100, 1000]:\n",
    "    ax.plot(xx, predict(xx, weight=output.loc[ind, 'weight'], bias=output.loc[ind, 'bias']), label='epoch {}'.format(output.loc[ind, 'epoch']))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f988c-fd80-4c26-976d-01057e709dee",
   "metadata": {},
   "source": [
    "Now let's see what happens when we change the starting point - that is, the initial guess for $w$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f26645-0c95-4f15-982e-d1c6ec3576f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "output = train(sun, tmax, weight=0, bias=0, learning_rate=1e-7, epochs=10000)\n",
    "\n",
    "output.tail(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d90ff-5aa4-40f7-951b-b7aeaacf68d4",
   "metadata": {},
   "source": [
    "From this, we can see that the values of both $w$ and $b$ are pretty far away from the optimal values - the value for $b$ has barely changed from the initial guess, and the value for $w$ has converged toward a less than optimal value. Not only that, but the average loss is much higher than we calculated for the optimal values of $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa4360-01e6-4c4b-9208-313cc64f557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(sun, tmax, '.')\n",
    "\n",
    "xx = np.arange(0, 351, 50)\n",
    "\n",
    "for ind in [0, 10, 50, 100, 1000]:\n",
    "    ax.plot(xx, predict(xx, weight=output.loc[ind, 'weight'], bias=output.loc[ind, 'bias']), label='epoch {}'.format(output.loc[ind, 'epoch']))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9239f2-e62d-4677-8490-6ec7f36ce621",
   "metadata": {},
   "source": [
    "## visualizing the loss surface\n",
    "\n",
    "To understand what's happened here, we'll look at something called the **loss surface** - that is, a way to visualize the shape of the loss function based on different parameter values. To begin, we'll create a function to calculate the loss for a range of different parameter values, then make a contour plot of the loss surface using `.contour()` ([documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.contour.html)). We'll also make a plot that shows cross-sections of the loss surface: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62bb24-6cb2-422d-aebc-6c1cb4a03a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_surface(weights, biases, xdata, ydata):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    loss_surf = []\n",
    "\n",
    "    for b in biases:\n",
    "        for w in weights:\n",
    "            loss_surf.append(avg_loss(xdata, ydata, w, b)) # \n",
    "\n",
    "    loss_surf = np.array(loss_surf).reshape(len(weights), len(biases)) # reshape so that the array is rectangular\n",
    "    W, B = np.meshgrid(weights, biases) # get a grid of weight and bias values\n",
    "\n",
    "    ax.contour(W, B, loss_surf) # plot contours of the loss surface\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_cross_sections(weights, biases, xdata, ydata):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    ax1, ax2 = axs\n",
    "\n",
    "    losses_w = np.array([avg_loss(xdata, ydata, w, 0) for w in weights])\n",
    "    losses_b = np.array([avg_loss(xdata, ydata, 0, b) for b in biases])\n",
    "    \n",
    "    ax1.plot(weights, losses_w, 'k')\n",
    "    ax1.plot(0, avg_loss(xdata, ydata, 0, 0), 'ro')\n",
    "    ax1.set_ylabel('average loss, $l$')\n",
    "    ax1.set_xlabel('$w$')\n",
    "    \n",
    "    ax2.plot(biases, losses_b, 'k')\n",
    "    ax2.plot(0, avg_loss(xdata, ydata, 0, 0), 'ro')\n",
    "    ax2.set_xlabel('$b$')\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8344713-c4c4-42c6-ba91-bd580baf65e1",
   "metadata": {},
   "source": [
    "Now, let's look at the loss surface for values of $w$ that range between -0.2 and 0.2, and values of $b$ that range between -5 and 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29533c-b397-4734-8352-c0e746dc3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_loss_surface(np.linspace(-0.2, 0.2, 100), # 100 values between -0.2 and 0.2\n",
    "                            np.linspace(-5, 25, 100), # 100 values between -5 and 25\n",
    "                            sun, tmax)\n",
    "\n",
    "ax.plot(model.coef_, model.intercept_, 'rx', label='minimum value')\n",
    "# ax.plot(output.weight.values[::10], output.bias.values[::10], 'b.', label='calculated values')\n",
    "ax.plot(0, 0, 'ro', label='starting point')\n",
    "\n",
    "ax.set_xlabel('$w$')\n",
    "ax.set_ylabel('$b$')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bed244-6663-4eb1-ac77-b3b0e44a86c7",
   "metadata": {},
   "source": [
    "We can see that the surface is fairly lopsided - the range of $w$ values is much, much smaller than the range of $b$ values. Now, let's look at a cross section of the surface through our starting point, holding both $w$ and $b$ constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d389c0b-11d1-40a0-b393-5444e920ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_cross_sections(np.linspace(-0.2, 0.2, 100), np.linspace(-5, 25, 100), sun, tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883d68d-74c9-4f85-8f3f-30ebc82b71af",
   "metadata": {},
   "source": [
    "Part of the problem here is that there are big differences in the value of the partial derivatives of $l$ with respect to $w$ and $b$ are very different. The value of $\\partial l/\\partial w$ at $w=0$ is -3068.82, while the value of $\\partial l/\\partial b$ at $b=0$ is -26.28 - this means that when we multiply by the learning rate, we have a much larger change for $w$ compared to $b$. Large gradient values also mean that when the learning rate is too large, we end up \"jumping\" back and forth across the minimum, and can even end up failing to reach a minimum value at all. \n",
    "\n",
    "## scaling\n",
    "\n",
    "To help counteract this, we can **scale** our feature and label values. There are a number of ways to do this, but they typically involve subtracting the mean value and dividing by either the standard deviation, or by the range of the dataset (the difference between the maximum and minimum values):\n",
    "\n",
    "$$ x_s = \\frac{x - \\bar{x}}{x_{\\rm max} - x_{\\rm min}} $$\n",
    "\n",
    "First, we'll scale the values of `sun` and `tmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d06fa-34e5-4644-9ab1-7b48812a709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_scaled = (sun - sun.mean()) / (sun.max() - sun.min())\n",
    "tmax_scaled = (tmax - tmax.mean()) / (tmax.max() - tmax.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77748f0-bcef-42ae-bb2d-5e851ea7c4aa",
   "metadata": {},
   "source": [
    "Now, let's look at the shape of the loss surface using the scaled values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4b78d-86de-4370-9910-a56b8e257030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s = linear_model.LinearRegression()\n",
    "model_s.fit(sun_scaled, tmax_scaled) # fit the scaled data\n",
    "\n",
    "fig, ax = plot_loss_surface(np.linspace(-1, 1, 100), # 100 values between -0.2 and 0.2\n",
    "                            np.linspace(-1, 1, 100), # 100 values between -5 and 25\n",
    "                            sun_scaled, tmax_scaled)\n",
    "\n",
    "ax.plot(model_s.coef_, model_s.intercept_, 'rx', label='minimum value') # plot the parameters calculated by scaling the data\n",
    "ax.plot(0, 0, 'ro', label='starting point') # plot the starting point\n",
    "\n",
    "ax.set_xlabel('$w_s$')\n",
    "ax.set_ylabel('$b_s$')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9e68a-7905-4300-bf9d-1b3ec599dd2c",
   "metadata": {},
   "source": [
    "Notice that the surface is much less lopsided now - the slope of the surface in the $b_s$ direction is greater than the slope of the surface in the $w_s$ direction, but $w_s$ and $b_s$ are at least the same order of magnitude.\n",
    "\n",
    "Looking at the cross-section of values, we can see the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5063c1f-9a60-4114-9a0f-62629e9b0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_cross_sections(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100), sun_scaled, tmax_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998c4d6-9d1b-4ab1-8e7d-2245d236782a",
   "metadata": {},
   "source": [
    "Using the scaled data, the value of $\\partial l/\\partial w$ at $w=0$ is -0.064, while the value of $\\partial l/\\partial b$ at $b=0$ is ~0 (because we have scaled *and* centered the data). \n",
    "\n",
    "This has two practical effects: first, it means that we can use much larger learning rates; and second, we should see that the changes are more even at each epoch, as opposed to seeing big changes in $w$ and almost no change in $b$.\n",
    "\n",
    "In order to get the values of $w$ and $b$ in the original units of the data, we need to convert them. Fortunately, we can do this by re-arranging the following equation so that it is in the form $y = wx + b$:\n",
    "\n",
    "$$ \\frac{y - \\bar{y}}{y_{\\rm max} - y_{\\rm min}} = w_s \\frac{x - \\bar{x}}{x_{\\rm max} - x_{\\rm min}} + b_s $$\n",
    "\n",
    "When we do this, we should get the following values for $w$ and $b$:\n",
    "\n",
    "$$ w = \\frac{y_{\\rm max} - y_{\\rm min}}{x_{\\rm max} - x_{\\rm min}} w_s $$\n",
    "\n",
    "$$ b = b_s (y_{\\rm max} - y_{\\rm min}) + \\bar{y} - \\frac{y_{\\rm max} - y_{\\rm min}}{x_{\\rm max} - x_{\\rm min}} w_s \\bar{x} $$\n",
    "\n",
    "In the function defined in the cell below, we have made a number of changes. First, we have added the option to scale the data using the `scale` argument (by default, `scale=False`). If we scale the data, we make sure to record the un-scaled values of $w$ and $b$, using the equations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee8e4f-862f-4260-b4ed-3a86b59a2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xdata, ydata, weight, bias, learning_rate, epochs, plot=True, scale=False):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    if scale:\n",
    "        xx = (xdata - xdata.mean()) / (xdata.max() - xdata.min())\n",
    "        yy = (ydata - ydata.mean()) / (ydata.max() - ydata.min())\n",
    "    else:\n",
    "        xx = xdata\n",
    "        yy = ydata\n",
    "    \n",
    "    for ee in range(epochs):\n",
    "        weight, bias = update_parameters(xx, yy, weight, bias, learning_rate)\n",
    "\n",
    "        if ee % 10 == 0:\n",
    "            if scale:\n",
    "                df.loc[ee, 'weight'] = weight * (ydata.max() - ydata.min()) / (xdata.max() - xdata.min())\n",
    "                df.loc[ee, 'bias'] = bias * (ydata.max() - ydata.min()) + ydata.mean() - (ydata.max() - ydata.min()) / (xdata.max() - xdata.min()) * weight * xdata.mean()\n",
    "            else:\n",
    "                df.loc[ee, 'weight'] = weight\n",
    "                df.loc[ee, 'bias'] = bias\n",
    "\n",
    "            df.loc[ee, 'avg_loss'] = avg_loss(xdata, ydata, df.loc[ee, 'weight'], df.loc[ee, 'bias'])\n",
    " \n",
    "    if scale:\n",
    "        df.loc[ee, 'weight'] = weight * (ydata.max() - ydata.min()) / (xdata.max() - xdata.min())\n",
    "        df.loc[ee, 'bias'] = bias * (ydata.max() - ydata.min()) + ydata.mean() - (ydata.max() - ydata.min()) / (xdata.max() - xdata.min()) * weight * xdata.mean()\n",
    "    else:\n",
    "        df.loc[ee, 'weight'] = weight\n",
    "        df.loc[ee, 'bias'] = bias\n",
    "    \n",
    "    df.loc[ee, 'avg_loss'] = avg_loss(xdata, ydata, df.loc[ee, 'weight'], df.loc[ee, 'bias'])\n",
    "    \n",
    "    if plot:\n",
    "        ax = df.reset_index(names='epoch').plot('epoch', 'avg_loss', legend=False)\n",
    "        ax.set_ylabel('average loss')\n",
    "        \n",
    "    \n",
    "    return df.reset_index(names=['epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada2034-da69-4068-80ac-8050b134973f",
   "metadata": {},
   "source": [
    "Now, let's see how well this works by using a learning rate of 0.1, and training the model for only 1000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf136871-9bff-4cf9-b136-d4fb2774586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = train(sun, tmax, weight=0, bias=0, learning_rate=0.1, epochs=1000, scale=True)\n",
    "output.tail(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cba2f-b22a-4847-91a0-e10714565192",
   "metadata": {},
   "source": [
    "Not bad - after only 1000 epochs, we have values for $w$ and $b$ that very nearly match the values calculated using `LinearRegression.fit()`. Try changing the values of `learning_rate` and `epochs` to see how close to the \"true\" value you can come. How large of a learning rate can you have before you fail to get a \"good\" solution?\n",
    "\n",
    "The cell below will plot the regression lines using the parameters from a few different intermediate epochs, along with the linear fit calculated using `LinearRegression.fit()` - as before, we should see that there's rapid improvement followed by slow improvement, but the end result is almost indistinguishable from the \"true\" solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3421a-19ea-4800-bc27-7d59a501a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(sun, tmax, '.')\n",
    "\n",
    "xx = np.arange(0, 351, 50)\n",
    "\n",
    "for ind in [0, 10, 100]:\n",
    "    ax.plot(xx, predict(xx, weight=output.loc[ind, 'weight'], bias=output.loc[ind, 'bias']), label='epoch {}'.format(output.loc[ind, 'epoch']))\n",
    "\n",
    "ax.plot(xx, model.predict(xx.reshape(-1, 1)), 'k--', label='ols linear fit')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f004f7f-b1d9-43fe-9832-11d6ccb7671f",
   "metadata": {},
   "source": [
    "## next steps\n",
    "\n",
    "That's all for this exercise. In this exercise, we have seen the basics of how we can train a model using `scikit-learn`. We have also used the example of linear regression to illustrate how training a model works in the background, by showing how we can tune different **hyperparameters** to find the optimal model parameters. We have also seen how important it can be to scale our data when training a model - it enables us to use larger **learning rate** values and spend less time training a model; it may also make it possible for the model to converge to a solution.\n",
    "\n",
    "There are some machine learning algorithms for which scaling input data is essential - we will cover these more as we continue through the workshop.\n",
    "\n",
    "For now, try at least one of the following exercises:\n",
    "\n",
    "- Train a model for the relationship between `tmax` and `tmin`, with and without scaling. Does scaling make a large difference to the final result? Why or why not? \n",
    "- Train a model for the relationship between `sun` and `rain`, again with/without scaling. Does scaling make a large difference to the final result? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
