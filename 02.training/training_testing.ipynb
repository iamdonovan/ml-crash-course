{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1571569d-321c-4138-8e9c-9782e65bcccc",
   "metadata": {},
   "source": [
    "# training, testing, and validation\n",
    "\n",
    "In this exercise, we'll have a look at some more fundamental concepts of machine learning, including:\n",
    "\n",
    "- standarization (similar to normalization seen previously)\n",
    "- splitting a dataset into training and testing (+validation) partitions\n",
    "- overfitting/underfitting + generalization\n",
    "- kernel functions and the \"kernel trick\"\n",
    "\n",
    "We'll look at all of this while also introducing **support vector machine** (SVM) classification, a type of learning model that seeks to classify data into one of two (or sometimes more) categories.\n",
    "\n",
    "## data\n",
    "\n",
    "The goal for this exercise will be to see if we can determine whether a location is rainier or drier than average, based on other meteorological parameters: namely, the average daily minimum temperature (`tmin`), and the average monthly hours of sunshine (`sun`).\n",
    "\n",
    "We'll do this using a combination of historic meteorological observations from the [UK Met Office](https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data), combined with climated averages from between 1971-2000 obtained from [Met Éireann](https://data.gov.ie/dataset/met-eireann-1971-2000-climate-averages).\n",
    "\n",
    "## importing libraries\n",
    "\n",
    "Before getting started, we will import the libraries (packages) that we will use in the exercise:\n",
    "\n",
    "- [sklearn](https://scikit-learn.org/), for preprocessing our data and doing the SVM classification;\n",
    "- [pandas](https://pandas.pydata.org/), for reading the data from a file;\n",
    "- [numpy](https://numpy.org/), for working with arrays;\n",
    "- [matplotlib](https://matplotlib.org/), for making plots.\n",
    "\n",
    "Remember that to do this, we use the `import` statement, followed by the name of the package. We can also use `from` to import part of a package, and we can *alias* the package name using `as`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f38fa-7c16-485c-97de-c5551336cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model, model_selection, preprocessing, svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c07ad-2ccb-4c72-b880-4da1bcfece28",
   "metadata": {},
   "source": [
    "Next, we use `pd.read_csv()` to read the CSV file of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0cefe6-b6c5-4639-a4a5-82e329af5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/annual_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22340980-2170-4ca1-9af6-5756c509c01c",
   "metadata": {},
   "source": [
    "Next, let's have a look at the data using `.head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfbbbc-2488-4fd0-8880-8fe755ad2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # show the first 5 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434cff5-cf00-41f8-ba36-89bd49a25555",
   "metadata": {},
   "source": [
    "Here, each row corresponds to a station, with features including:\n",
    "\n",
    "- `tmax`: average daily maximum temperature in °C;\n",
    "- `tmin`: average daily minimum temperature in °C;\n",
    "- `air_frost`: average annual days of air frost (only for UK stations);\n",
    "- `rain`: average monthly precipitation in mm;\n",
    "- `sun`: average monthly hours of sun;\n",
    "- `classification`: whether the station gets more (`rainy`) or less (`dry`) precipitation than average.\n",
    "\n",
    "Before jumping into the machine learning part of the exercise, we'll first use `matplotlib` to make a plot showing `sun` vs `tmin`, colored by `classification. This can help show us whether or not we actually *can* separate rainy/dry locations using these two features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66aa5e9-396d-46df-bd60-a8073aec80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1) # create a figure object with a single axis\n",
    "\n",
    "colors = {'dry': 'orangered', 'rainy': 'royalblue'} # create a dict of color \n",
    "\n",
    "# for each value of classification, plot sun vs tmin using the colors defined above\n",
    "for name, group in data.groupby('classification'):\n",
    "    ax.scatter(group.tmin, group.sun, edgecolor='k', c=colors[name], label=name)\n",
    "\n",
    "# set the axis labels\n",
    "ax.set_xlabel('avg min temp (°C)')\n",
    "ax.set_ylabel('avg hours of sun')\n",
    "\n",
    "# add a legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17966aee-ee8a-4cbc-bfe1-36a5161e92de",
   "metadata": {},
   "source": [
    "From the plot, we can see that it might be possible to draw a line separating the two groups, but it's made more difficult by noise: the two groups are not nicely separated. That's okay, because we will discuss a few techniques to help deal with noise in our input data.\n",
    "\n",
    "## preparing the data: standardizing\n",
    "\n",
    "Have a look at the values of `sun` on the y axis (90 to 150 hours), vs the values of `tmin` on the x axis (3 to 8 °C). Like we saw in the previous exercise, having features with vastly different values can be a problem for many machine learning algorithms. In particular, if the variance of one feature is much larger than others, it may dominate the objective function and cause the model to be unable to learn correctly from the other features - similar to what we saw with the linear regression example.\n",
    "\n",
    "For SVM models, we want to make sure that the data are *centered* (i.e., the mean value is 0), with a unit variance (i.e., the variance is equal to one). This helps ensure that the model is able to use all of the features to learn from evenly.\n",
    "\n",
    "First, we'll create an *array* of feature values by indexing `tmin` and `sun` from the **DataFrame**, then using `.to_numpy()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html)) to return an $N\\times 2$ array of values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e64e11-8eb1-4f98-981a-9b660ad01f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['tmin', 'sun']].to_numpy() # create the feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8c2ad-1a44-401d-b271-f89e01263abf",
   "metadata": {},
   "source": [
    "SVM, like most machine learning algorithms, requires that our labels are *numeric* - that is, we can't use text like *dry* and *rainy* as labels, we need to convert these to numbers. We'll cover more sophisticated ways to encode text features when we cover feature engineering in the next exercise. For now, we can use `.map()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html)) along with a **dict** to replace `'dry'` with a value of `-1` and `'rainy'` with a value of `1`, then use `.to_numpy()` to convert this to a `numpy` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790ca27-254d-44b4-98aa-6a99415e7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['classification'].map({'dry': -1, 'rainy': 1}).to_numpy() # create a vector of numeric labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c7194-d618-4abe-9f2d-51b6d94baa2f",
   "metadata": {},
   "source": [
    "In order to *standardize* the features, we first create a **StandardScaler** object ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)), then use `.fit()` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit)) to compute the mean and standard deviation to use for scaling.\n",
    "\n",
    "Then, we use `.transform()` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.transform)) to actually standardize, using the calculated mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f69c6-2a82-4a1e-b022-13d7e824a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(features) # calculate the mean and standard deviation using fit\n",
    "scaled_features = scaler.transform(features) # apply the standardization to the feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d95a4-6033-4c57-8c33-563fc2ee423f",
   "metadata": {},
   "source": [
    "Now, we should see that the mean of each vector is (very nearly) zero, using `.mean()` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2aa2b7-fbcc-4b53-aa4d-82c87da4128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features.mean(axis=0) # mean should be very nearly zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a3f78-0964-414c-8ba6-287fe9e48690",
   "metadata": {},
   "source": [
    "And, we can use `.var()` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.var.html)) to check that the variance of each vector is equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a56bda-aeae-40c0-8e70-7033ba257232",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features.var(axis=0) # variance should be nearly equal to one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f650e1-6dab-4ced-bdad-8fc9e0a889f9",
   "metadata": {},
   "source": [
    "Now we can plot the scaled values of `tmin` and `sun`, colored by the numeric labels, to see that the data should still look the same - just scaled differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eec4d8-6dab-4bd3-a19b-6ab00909d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.scatter(scaled_features[labels == -1, 0], scaled_features[labels == -1, 1], s=80, c='orangered', edgecolors='k', label='dry')\n",
    "ax.scatter(scaled_features[labels == 1, 0], scaled_features[labels == 1, 1], s=80, c='royalblue', edgecolors='k', label='rainy')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('avg min temp (°C)')\n",
    "ax.set_ylabel('avg hours of sun')\n",
    "\n",
    "ax.set(xlim=(-3.5, 3.5), ylim=(-3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9058a-3e9f-4323-9e4c-027f0ef58494",
   "metadata": {},
   "source": [
    "## preparing the data: splitting/partitioning\n",
    "\n",
    "Now that we've standardized our features and converted our text labels into numbers, we are nearly ready to train the classifier. I say nearly ready, because there's one last important step: **partitioning** our data into training, testing, and valdiation datasets.\n",
    "\n",
    "The reason that we do this is that we want to be able to evaluate how well the model has learned. We want to avoid having the model only do well at predicting labels for examples it has already seen - we want to make sure that the model **generalizes** to previously unseen data, so that we know it isn't simply regurgitating the answers to questions it has memorized.\n",
    "\n",
    "In practice, we use the **training** dataset to actually build the model - this is the data that the model \"learns\" the parameters for. The **validation** dataset is what we use to help determine which learning algorithm to use, and to find the best values of model hyperparameters, and the **testing** dataset is how we assess the model performance once we are sure we have found the optimal model hyperparameters. The training dataset is typically the largest of the three, while the testing and validation datasets are typically the same size. How big each partition is depends on the size of the dataset - a common rule of thumb is to use 70% of the data for training and 15% each for testing and validation; with extremely large datasets, the split might be more extreme.\n",
    "\n",
    "We can use `model_selection.train_test_split()` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) to actually do the split - note that in order to get three partitions (training, testing, and validation), we need to run this split twice - first to get the training partition, then again to get the testing and validation partitions:\n",
    "\n",
    "```python\n",
    "# use 70% of the data for training, 15% for testing, 15% for validation\n",
    "X_train, X_, y_train, y_ = model_selection.train_test_split(scaled_features, labels, test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = model_selection.train_test_split(X_, y_, test_size=0.5)\n",
    "```\n",
    "\n",
    "Because we have a small dataset ($n=49$), we're only going to split into training and testing partitions, because we want to make sure that we have enough observations in our dataset to get a good picture of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffeda43-ff22-4429-8eb2-de45e9f172a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 70% of the data for training, 15% for testing, 15% for validation\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(scaled_features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7a9b4-cfd3-46bc-a9c3-3b0f6223ae8e",
   "metadata": {},
   "source": [
    "Note that by specifying the `random_state` argument, we ensure that the split will be the same each time we run this (even on other computers). In practice, we may want to have a truly random split each time we run the data - in this case, we would omit this argument.\n",
    "\n",
    "Now, we can plot the two datasets side-by-side to compare them. Hopefully, they should look fairly similar - if not, we may have a more challenging time training and evaluating our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c99ac-b255-4c8a-b9c4-e055467f4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "for ind, XY in enumerate(zip([X_train, X_test], [y_train, y_test])):\n",
    "    X_, y_ = XY\n",
    "    axs[ind].scatter(X_[y_ == -1, 0], X_[y_ == -1, 1], s=80, c='orangered', edgecolors='k', label='dry')\n",
    "    axs[ind].scatter(X_[y_ == 1, 0], X_[y_ == 1, 1], s=80, c='royalblue', edgecolors='k', label='rainy')\n",
    "    \n",
    "    axs[ind].set_xlabel('avg min temp (°C)')\n",
    "\n",
    "axs[0].set_ylabel('avg hours of sun')\n",
    "axs[0].set_title('training data')\n",
    "\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('testing data')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set(xlim=(-3.5, 3.5), ylim=(-3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e2573-9475-4414-9646-dc88597e1502",
   "metadata": {},
   "source": [
    "## support vector machine\n",
    "\n",
    "Now that we have partitioned our dataset, we're ready to train our model. In this exercise, we'll explore using support vector machine (SVM) classification. In its basic form, SVM is trying to find parameters $\\mathbf{w}$ and $b$ such that:\n",
    "\n",
    "$$ \\mathbf{wx}_i - b \\geq 1, y_i = 1 $$\n",
    "\n",
    "$$ \\mathbf{wx}_i - b \\leq -1, y_i = -1 $$\n",
    "\n",
    "where $\\mathbf{w}$ and $\\mathbf{x}_i$ are vectors with dimension equal to the number of features in our dataset, and $y_i$ is the label for the feature vector $\\mathbf{x}_i$. \n",
    "\n",
    "Alternatively, we can write the constraints like this:\n",
    "\n",
    "$$ y_i (\\mathbf{wx}_i - b) \\geq 1 $$\n",
    "\n",
    "Another way of saying all of this is that we're trying to find a **hyperplane**, also called the **decision boundary**, that separates two classes in feature space by the largest **margin** (distance between the closest examples of each class) possible. To do this, we need to minimize the **Euclidean norm** of $\\mathbf{w}$, $\\lVert\\mathbf{w}\\rVert$:\n",
    "\n",
    "$$ \\lVert\\mathbf{w}\\rVert = \\sqrt{\\sum_j w_j^2} $$\n",
    "\n",
    "Because the distance between the two hyperplanes defined by $\\mathbf{wx}_i - b = \\pm 1$ is $2 / \\lVert\\mathbf{w}\\rVert$, by minimizing $\\lVert\\mathbf{w}\\rVert$, we get the maximum distance between the two planes. We'll stick to two dimensions for now because it's easier to visualize - in this case, what we're trying to find is a line that separates the two classes. \n",
    "\n",
    "### hinge loss\n",
    "\n",
    "As we can see above, there is no line that will cleanly separate these two classes - in this case, we use something called the **hinge loss** function:\n",
    "\n",
    "$$ \\max(0, 1 - y_i (\\mathbf{wx}_i - b)) $$\n",
    "\n",
    "Values that are on the \"correct\" side of the decision boundary have a loss of 0; values that are on the \"wrong\" side have a loss that is proportional to the distance from the decision boundary. In this case, the cost function looks like:\n",
    "\n",
    "$$ C \\lVert\\mathbf{w}\\rVert^2 + \\frac{1}{N}\\sum_i \\max(0, 1 - y_i (\\mathbf{wx}_i - b)) $$\n",
    "\n",
    "where $C$ is a *positive* hyperparameter that determines the tradeoff between increasing the size of the decision boundary and ensuring that each feature $\\mathbf{x}_i$ is on the correct side of the decision boundary. Increasing the value of $C$ means that we place less emphasis on misclassification; decreasing the value of $C$ means that we have a smaller margin size. We will come back to this more in later exercises, when we talk about something called **regularization**.\n",
    "\n",
    "### using scikit-learn\n",
    "\n",
    "To do this using `scikit-learn`, we need to first create an object of the **class** corresponding to our machine learning algorithm - in this case, we're using support vector classification, so we use `svm.SVC` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)). In the documentation, you can see that one of the input arguments to `svm.SVC()` is `C` - the \"regularization parameter\" discussed in the previous paragraphs. For now, we will stick with the default value of `1.0`, but in later exercises we will experiment with changing this.\n",
    "\n",
    "As we will explore more later, we can also specify what kind of `kernel` to use for the classifier - to start, we'll use the \"classic\" version, where the decision boundary is linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5df679-776a-4e79-bcd9-81597fb1c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf = svm.SVC(kernel='linear') # create an SVC object with a linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f2cbe-bb19-452d-9cfa-8e80266f909c",
   "metadata": {},
   "source": [
    "Just like with `LinearRegression()`, we use the `.fit()` method ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)) to train the model using our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c1f2e-14f7-4be0-a2cb-ac26f8dfc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf.fit(X_train, y_train) # train the classifier using the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131732e-7c97-4399-9ed7-ea7273a2307e",
   "metadata": {},
   "source": [
    "And that's it. To view the decision boundary, we can write the following function, which uses `.decision_function()` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function)) along with a grid of feature values to create a mesh that displays the location of the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6268e6-882c-4ed6-9d73-a8d4774822e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_surface(X, y, X_test, clf):\n",
    "    cmap = ListedColormap([\"orangered\",\"royalblue\"], name='from_list', N=None)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # plot the input data\n",
    "    ax.scatter(X[y == -1, 0], X[y == -1, 1], c='orangered', zorder=10, edgecolor='k', s=20, label='dry')\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='royalblue', zorder=10, edgecolor='k', s=20, label='rainy')\n",
    "\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c='none', zorder=8, edgecolor='k', s=80, label='testing data')\n",
    "    # create a grid of feature value points to show the decision boundary\n",
    "    XX, YY = np.mgrid[-3.5:3.5:200j, -3:3:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "    Z = Z.reshape(XX.shape)\n",
    "\n",
    "    # show the decision boundary\n",
    "    ax.pcolormesh(XX, YY, Z > 0, cmap=cmap)\n",
    "    \n",
    "    # show the contours\n",
    "    ax.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-0.5, 0, 0.5])\n",
    "    \n",
    "    ax.set(xlim=(-3.5, 3.5), ylim=(-3, 3))\n",
    "    \n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4221665-b134-4cb6-9a04-667ecda19a88",
   "metadata": {},
   "source": [
    "When we run this function, we can see how the classifier has done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bedfd-807d-4a8b-8fff-af4d14e8e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_decision_surface(scaled_features, labels, X_test, linear_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7753cf-e3e7-4c15-a743-bb2d1f74b6b4",
   "metadata": {},
   "source": [
    "As we identified when we first plotted the data, it is difficult to fully separate these two classes by a straight line - there's some overlap between them. What we do see with the training data (small individual circles) is that the majority of the points lie on the correct side of the line, even if there are a number of \"rainy\" points that would be classified as dry using the classifier, and a few \"dry\" points that would be classified as rainy.\n",
    "\n",
    "## checking the results\n",
    "\n",
    "We'll cover formal ways to assess model performance more in the next exercise, but for now we can look at the **mean accuracy** using `.score()` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.score)) - this is the proportion of labels that the model correctly predicted. For our training data, we can see that the proportion is fairly high, though not perfect - around 83%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756684d-7458-4892-9ac2-a991e4939558",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf.score(X_train, y_train) # get the mean accuracy for the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce90da-8f13-457c-bae0-0610e59438c9",
   "metadata": {},
   "source": [
    "We can also test the score using our test data - here, we can see that it's a bit lower at 80%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d230e45-4abf-4772-a4fe-28a2d2c7f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf.score(X_test, y_test) # get the mean accuracy for the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ee884-90a5-4a2e-8edc-497e3dccdac1",
   "metadata": {},
   "source": [
    "## generalization\n",
    "\n",
    "Ideally, we would like to see our model perform about the same on both the training data and the testing/validation data. When this is the case, as it is here, it's a good indication that our model **generalizes** well - that it's not simply regurgitating \"answers\" that it memorized during the training process. \n",
    "\n",
    "In machine learning, we are ultimately more interested in minimizing the loss on unseen or new data than we are in minimizing the loss on our training data, because it's the unseen data that our model will be used to help classify. If our model fails to generalize, it will be far less useful in practice.\n",
    "\n",
    "This leads us to two other concepts: **underfitting** and **overfitting**. A model that **underfits** doesn't do a very good job predicting the labels of the training data. This can happen for a variety of reasons, but the most common are that the model is too simple for the data, or the features that we are using are poor predictors for our labels. In the first case, we might need to consider a more complicated model; in the second case, we may need to add features that have a more meaningful relationship (\"higher predictive power\") with the label.\n",
    "\n",
    "A model that **overfits** typically does a very good job of predicting using the training data, but a very poor job when it sees data that it has not been trained on. As with underfitting, this can happen for a variety of reasons. The most common reasons are that the model is too complex for the data, or that we have too many features and not enough training examples. In the first case, we would need to consider a simpler model; in the second, we need to reduce the number of features, or increase the number of training examples that we have.\n",
    "\n",
    "As we continue through the rest of this workshop, I will try to highlight examples of problems where the model is overfit or underfit, and illustrate potential solutions to these problems.\n",
    "\n",
    "## the \"kernel trick\"\n",
    "\n",
    "Very often with real datasets, we will be unable to find a \"clean\" split between classes - we will either have **noise** in our input data, or we will have some kind of inherent non-linearity in our data. We have already seen an example of how SVM can be used in the first case, by introducing a tradeoff between the margin size and the \"cost\" of misclassification. This doesn't normally suffice when we have inherent non-linearity in our data, but there is an alternative solution: very often, we can transform our original space into a higher-dimensional space where there is a \"nice\" linear separation between classes.\n",
    "\n",
    "The problem with this is that it can be very costly to find an appropriate transformation - we don't know what this transformation looks like beforehand, which means we would need to try many such transformations and train our classifier in each of them.\n",
    "\n",
    "Fortunately, we don't actually have to do this in practice - instead, we can use something known as the **kernel trick**. By using something known as a **kernel function** (or just a **kernel**), we can efficiently and implicitly transform our input features into a higher-dimensional space.\n",
    "\n",
    "We will not go into the mathematical details of how this all works here - instead, we will look at some examples of different **kernel functions**, and see how this impacts the classifier training.\n",
    "\n",
    "### radial basis function kernel\n",
    "\n",
    "One of the most common kernel functions in practice is the **radial basis function** (**RBF**) kernel - indeed, this is actually the default kernel for `svm.SVC`. The RBF kernel has the form:\n",
    "\n",
    "$$ k(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp(-\\gamma\\cdot \\lVert\\mathbf{x}_1 - \\mathbf{x}_2\\rVert^2) $$\n",
    "\n",
    "where $\\lVert\\mathbf{x}_1 - \\mathbf{x}_2\\rVert^2$ is the squared **Euclidean distance** between two vectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$. Varying the value of the hyperparameter $\\gamma$ (`gamma`) helps determine whether the decision boundary in the original space is curved or smooth by changing the influence of each training sample on the decision boundary.\n",
    "\n",
    "In the example below, we first use `svm.SVC` to create a new classifier object, using the `kernel` argument to specify an RBF kernel. We'll use the default value of `gamma='scale'`, which means that the value of `gamma` is calculated as $1 / N\\sigma_X$, where $N$ is the number of features and $\\sigma_X$ is the variance of the input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42924e95-88d9-4547-9dfc-7b151d44eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_clf = svm.SVC(kernel='rbf') # create a new classifier object with an RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c5975-ad37-40ad-b4e1-45114299f8ac",
   "metadata": {},
   "source": [
    "As before, we train the classifier using `.fit()` and our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eb1bf-3b93-46a0-977b-864870cb2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_clf.fit(X_train, y_train) # train the classifier with the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9436eb-b69e-4f65-a30c-b8f18470e0f8",
   "metadata": {},
   "source": [
    "And now, we can use `plot_decision_surface()` to show the decision boundary along with the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658afef1-9139-474e-aba8-1b90a0a9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_decision_surface(scaled_features, labels, X_test, rbf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f83e6-8b9d-446f-ad91-d92f982ce0d5",
   "metadata": {},
   "source": [
    "Here, we see that the shape of the decision boundary is very different to the linear model. We can also see that while the results look fairly good for the training data, there are more testing data that are on the \"wrong\" side of the decision boundary.\n",
    "\n",
    "To check this, we can look at the mean accuracy of the training data using `.score()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5f3cf-7b7c-4e11-9918-dfae29fced02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0139296-06bf-4d0f-a40c-545f89b9dc7c",
   "metadata": {},
   "source": [
    "and for the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374a5ab-4056-4690-b90c-d029af0be309",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a95a5-173e-4ad1-a26b-43fe0a043186",
   "metadata": {},
   "source": [
    "So, we have a small improvement in the mean accuracy for the training data (0.833 vs 0.829), but it has come with a large decrease in the mean accuracy for the testing data: 0.533 vs 0.8. This is strong indication that our model has **overfit** the training data: the performance on \"new\" data (the testing data) is much worse than what we saw in training, indicating that the model has learned the peculiarities of the training data. \n",
    "\n",
    "With additional training data, we might be able to improve performance in both training and testing; for now, though, we'll leave this and look at a different kernel example.\n",
    "\n",
    "### polynomial kernels\n",
    "\n",
    "The next kernel we will look at is the **polynomial kernel**, which has the form:\n",
    "\n",
    "$$ k(\\mathbf{x}_1, \\mathbf{x}_2) = (\\gamma\\cdot\\mathbf{x}_1^\\top\\mathbf{x}_2 + r)^d $$\n",
    "\n",
    "where $d$ is the degree (`degree`) of the polynomial, $\\gamma$ (`gamma`) controls how much each training sample influences the shape of the decision boundary and $r$ is the bias term (`coef0`) that shifts the value of the polynomial up or down. By default with `svm.SVC`, `gamma` is calculated as $1 / N\\sigma_X$; `degree` equals 3 (i.e., a cubic polynomial); and `coef0` equals 0.\n",
    "\n",
    "As before, we'll use the default values to begin, but feel free to vary the different hyperparameters to see if you are able to improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6d6a6-5ad8-455c-aef5-8dc7fd1b9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_clf = svm.SVC(kernel='poly') # create a new classifier object with a third-degree polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca5586-e025-4b82-85d5-6d27f0a2a4ff",
   "metadata": {},
   "source": [
    "As before, we train the classifier using `.fit()` and our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2179a-99c2-4abe-b4e1-7750090856e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_clf.fit(X_train, y_train) # train the classifier with the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb62e5-93b8-4253-99ec-b92311bad5c0",
   "metadata": {},
   "source": [
    "And now, we can use `plot_decision_surface()` to show the decision boundary along with the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae2b3f-d2b4-4172-bd9c-bd6ec14db9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_decision_surface(scaled_features, labels, X_test, poly_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ebc8-7011-4a81-94c5-0dd99eb40a0b",
   "metadata": {},
   "source": [
    "Here, we see that the shape of the decision boundary is more similar to the linear model than what we saw with the RBF kernel. We can also see that while the results look fairly good for the training data, there are a lot more testing data that are on the \"wrong\" side of the decision boundary.\n",
    "\n",
    "Now, let's look at the mean accuracy of the training data using `.score()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e803a6-0ef5-48e2-be2e-e33e9f6be671",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_clf.score(X_train, y_train) # calculate the mean accuracy of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2717b-9653-47d3-beaf-b6d6bbce125b",
   "metadata": {},
   "source": [
    "The results here are slightly worse than the accuracy we obtained with both the linear and RBF kernels, though not by much. The testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15668c08-f8d5-4f7c-a915-a1ab8a7db00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_clf.score(X_test, y_test) # calculate the mean accuracy of the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382047db-cfca-4d83-8371-d9868a9ee772",
   "metadata": {},
   "source": [
    "show a similar result as the RBF kernel - the model has overfit the training data, and as a result, the accuracy using the testing data is quite poor.\n",
    "\n",
    "### the sigmoid kernel\n",
    "\n",
    "The final kernel function that we will look at is the **sigmoid** kernel function, which has the form:\n",
    "\n",
    "$$ k(\\mathbf{x}_1, \\mathbf{x}_2) = \\tanh(\\gamma\\cdot\\mathbf{x}_1^\\top\\mathbf{x}_2 + r) $$\n",
    "\n",
    "where $\\tanh$ is the [hyperbolic tangent function](https://en.wikipedia.org/wiki/Hyperbolic_functions#Definitions). As with the polynomial kernel, $\\gamma$ (`gamma`) controls how much each training sample influences the shape of the decision boundary and $r$ is the bias term (`coef0`) that shifts the data up or down.\n",
    "\n",
    "As before, we will use the default values of `gamma` and `coef0`, but feel free to experiment with different values later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4828f9b-7d10-4a61-84ce-f45fc7f6ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_clf = svm.SVC(kernel='sigmoid') # create a new classifier object with a sigmoid kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a2423-b954-4247-b462-c3caf99495ce",
   "metadata": {},
   "source": [
    "Next, train the classifier using `.fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0112f5-018f-468f-97ce-58eaf61ead7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_clf.fit(X_train, y_train) # train the classifier with the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa61b4-75e8-40c7-b16a-8607f6e5611b",
   "metadata": {},
   "source": [
    "and use `plot_decision_surface()` to show the shape of the decision boundary along with the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe7753-ab0c-4b63-937a-6596b3308ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_decision_surface(scaled_features, labels, X_test, sig_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3cc4c-44e3-4fde-9881-5dd4760dd18a",
   "metadata": {},
   "source": [
    "With the `sigmoid` kernel, it looks like we have a number of points on the \"wrong\" side of the decision boundary for both classes, indicating that the training score will likely be lower than the other examples. We can verify this using `.score()` and the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70c909-13ca-47ac-b6a1-d1ec3b8676dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_clf.score(X_train, y_train) # calculate the mean accuracy of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc587b-6d67-4e57-9b53-a649a486d79a",
   "metadata": {},
   "source": [
    "and for the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b99a6c-5189-492e-8207-f6c282b8dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_clf.score(X_test, y_test) # calculate the mean accuracy of the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d870b-f083-4691-bc8c-a5f8e528b84d",
   "metadata": {},
   "source": [
    "Here we see something interesting - the model had a higher mean accuracy on the testing data than the training data, indicating that it generalized better than the other models, even if the training results were noticeably worse than the other models (i.e., it **underfit** the training data). After all of this, we can see that the original SVM formulation, with a linear decision boundary, has performed the best with our data, both in terms of training and testing. It turns out that despite their simplicity, linear models can be a very good choice for generalizing - especially when we don't have a strong reason to assume that our model needs to be more complex.\n",
    "\n",
    "## next steps\n",
    "\n",
    "That's all for this exercise. For additional practice, try at least one of the suggestions below \n",
    "\n",
    "- Try varying the different hyperparameters for at least one of the kernel functions (`rbf`, `poly`, or `sigmoid`) introduced above. Are you able to improve the training accuracy, as well as the testing accuracy?\n",
    "- Instead of using `tmax` as an input feature, try using `tmax` - does this improve, or worsen, the results? Why do you think this might be?\n",
    "- Try adding dimensionality by using `tmin`, `tmax`, and `sun` as input features. How does this change the results? Does it depend on the kernel function used?\n",
    "- Try different combinations of `tmax` and `tmin` (e.g., `tmax` - `tmin`, `tmax` + `tmin`, etc.). Are there kernel functions for which this is better or worse?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
